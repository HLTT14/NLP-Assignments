{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HLTT14/NLP-Assignments/blob/main/NLP_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqFlDg2jBoPt"
      },
      "source": [
        "# Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOI1rkvOLU0q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd579d7-5d29-461b-f115-e4b44bae1330"
      },
      "source": [
        "!git clone https://github.com/HLTT14/NLP-Assignments.git"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'NLP-Assignments' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCtP5yh4Ahv7"
      },
      "source": [
        "!unzip -o -q /content/NLP-Assignments/Assignment2/train.zip"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyZea-XFSvRa",
        "outputId": "7117af7c-3102-4ae4-f0cc-484af1458281"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Text-Mining/Persian-Wikipedia-Corpus/master/models/word2vec-cbow/word2vec.model-cbow-size%3D200-window%3D5.part1.rar"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-29 04:05:49--  https://raw.githubusercontent.com/Text-Mining/Persian-Wikipedia-Corpus/master/models/word2vec-cbow/word2vec.model-cbow-size%3D200-window%3D5.part1.rar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99614720 (95M) [application/octet-stream]\n",
            "Saving to: ‘word2vec.model-cbow-size=200-window=5.part1.rar’\n",
            "\n",
            "word2vec.model-cbow 100%[===================>]  95.00M   181MB/s    in 0.5s    \n",
            "\n",
            "2021-05-29 04:05:50 (181 MB/s) - ‘word2vec.model-cbow-size=200-window=5.part1.rar’ saved [99614720/99614720]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEzlU5SDTPxN",
        "outputId": "8176b7c2-3b60-49b0-ed4a-7e7a068abf14"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Text-Mining/Persian-Wikipedia-Corpus/master/models/word2vec-cbow/word2vec.model-cbow-size%3D200-window%3D5.part2.rar"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-29 04:06:25--  https://raw.githubusercontent.com/Text-Mining/Persian-Wikipedia-Corpus/master/models/word2vec-cbow/word2vec.model-cbow-size%3D200-window%3D5.part2.rar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82727486 (79M) [application/octet-stream]\n",
            "Saving to: ‘word2vec.model-cbow-size=200-window=5.part2.rar’\n",
            "\n",
            "word2vec.model-cbow 100%[===================>]  78.89M   171MB/s    in 0.5s    \n",
            "\n",
            "2021-05-29 04:06:26 (171 MB/s) - ‘word2vec.model-cbow-size=200-window=5.part2.rar’ saved [82727486/82727486]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrSkBtebTZ_7",
        "outputId": "ba5cdd67-e51d-431e-f4d0-f2ebb25fb4be"
      },
      "source": [
        "!unrar e /content/word2vec.model-cbow-size=200-window=5.part1.rar"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/word2vec.model-cbow-size=200-window=5.part1.rar\n",
            "\n",
            "Extracting  word2vec.model-cbow-size=200-window=5.bin                    \b\b\b\b  0%\b\b\b\b  1%\b\b\b\b  2%\b\b\b\b  3%\b\b\b\b  4%\b\b\b\b  5%\b\b\b\b  6%\b\b\b\b  7%\b\b\b\b  8%\b\b\b\b  9%\b\b\b\b 10%\b\b\b\b 11%\b\b\b\b 12%\b\b\b\b 13%\b\b\b\b 14%\b\b\b\b 15%\b\b\b\b 16%\b\b\b\b 17%\b\b\b\b 18%\b\b\b\b 19%\b\b\b\b 20%\b\b\b\b 21%\b\b\b\b 22%\b\b\b\b 23%\b\b\b\b 24%\b\b\b\b 25%\b\b\b\b 26%\b\b\b\b 27%\b\b\b\b 28%\b\b\b\b 29%\b\b\b\b 30%\b\b\b\b 31%\b\b\b\b 32%\b\b\b\b 33%\b\b\b\b 34%\b\b\b\b 35%\b\b\b\b 36%\b\b\b\b 37%\b\b\b\b 38%\b\b\b\b 39%\b\b\b\b 40%\b\b\b\b 41%\b\b\b\b 42%\b\b\b\b 43%\b\b\b\b 44%\b\b\b\b 45%\b\b\b\b 46%\b\b\b\b 47%\b\b\b\b 48%\b\b\b\b 49%\b\b\b\b 50%\b\b\b\b 51%\b\b\b\b 52%\b\b\b\b 53%\b\b\b\b 54%\n",
            "\n",
            "Extracting from /content/word2vec.model-cbow-size=200-window=5.part2.rar\n",
            "\n",
            "...         word2vec.model-cbow-size=200-window=5.bin                    \b\b\b\b 55%\b\b\b\b 56%\b\b\b\b 57%\b\b\b\b 58%\b\b\b\b 59%\b\b\b\b 60%\b\b\b\b 61%\b\b\b\b 62%\b\b\b\b 63%\b\b\b\b 64%\b\b\b\b 65%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b 68%\b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b 74%\b\b\b\b 75%\b\b\b\b 76%\b\b\b\b 77%\b\b\b\b 78%\b\b\b\b 79%\b\b\b\b 80%\b\b\b\b 81%\b\b\b\b 82%\b\b\b\b 83%\b\b\b\b 84%\b\b\b\b 85%\b\b\b\b 86%\b\b\b\b 87%\b\b\b\b 88%\b\b\b\b 89%\b\b\b\b 90%\b\b\b\b 91%\b\b\b\b 92%\b\b\b\b 93%\b\b\b\b 94%\b\b\b\b 95%\b\b\b\b 96%\b\b\b\b 97%\b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7TK0dMC08B"
      },
      "source": [
        "# Install the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35ERzPoXJ1fj",
        "outputId": "2207c497-686e-4fc0-fd84-1204c670c8ce"
      },
      "source": [
        "!pip install hazm"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /usr/local/lib/python3.7/dist-packages (from hazm) (0.2.1)\n",
            "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.7/dist-packages (from hazm) (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "youHI4cYMXzY",
        "outputId": "08d77893-9882-47ab-fb52-019d0bb3f935"
      },
      "source": [
        "!wget -nc https://github.com/sobhe/hazm/releases/download/v0.5/resources-0.5.zip"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘resources-0.5.zip’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOFuQ_-MMbuI",
        "outputId": "59eadb1b-4adb-4d44-9dd1-0838fd733aab"
      },
      "source": [
        "!unzip -o /content/resources-0.5.zip -d /content/resources/"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/resources-0.5.zip\n",
            "  inflating: /content/resources/chunker.model  \n",
            "  inflating: /content/resources/langModel.mco  \n",
            "  inflating: /content/resources/lib/liblinear-1.8.jar  \n",
            "  inflating: /content/resources/lib/libsvm.jar  \n",
            "  inflating: /content/resources/lib/log4j.jar  \n",
            "  inflating: /content/resources/malt.jar  \n",
            "  inflating: /content/resources/postagger.model  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpm1S4DGBVXi"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV_ezkUUBdOO"
      },
      "source": [
        "# import necessary libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "from hazm import POSTagger\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XY95js1A0kJ"
      },
      "source": [
        "# Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMvyZ2LzA3ru"
      },
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = \"/content/train.data\""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhKaTZDtKVfQ"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u19hHwCdDAta"
      },
      "source": [
        "# Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I84-RWtEDyFf"
      },
      "source": [
        "df = pd.read_fwf(data_path, header = None, names=['word'],skip_blank_lines=False)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Etlx2NrDvg"
      },
      "source": [
        "df = df.replace(np.nan, '', regex=True)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "j_PIvlMElxfu",
        "outputId": "be69a305-ebcd-4e4a-d59f-8b8618c53e28"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td># gen_negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>منبع gen_negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>: gen_negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>) gen_negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>مجلة gen_positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                word\n",
              "0     # gen_negative\n",
              "1  منبع gen_negative\n",
              "2     : gen_negative\n",
              "3     ) gen_negative\n",
              "4  مجلة gen_positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BwdWGF6UGJB"
      },
      "source": [
        "df['tag'] = df.apply(lambda row: 'N' if 'gen_negative' in row.word.split() else 'P', axis = 1)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WViM21hpV9JX"
      },
      "source": [
        "df['word'] = df.apply(lambda row: row.word.replace('gen_negative', '').replace('gen_positive', '').strip(), axis = 1)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fctbWyYvKYUY"
      },
      "source": [
        "tagger = POSTagger(model='/content/resources/postagger.model')"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JyutGHGM65Z"
      },
      "source": [
        "df['POS'] = df.apply(lambda row: tagger.tag([row.word]), axis = 1)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-Ok7HlR8Z5l"
      },
      "source": [
        "X = [] # store input sequence\n",
        "Y = [] # store output sequence\n",
        "X_sentence = []\n",
        "Y_sentence = []\n",
        "for index, row in df.iterrows():\n",
        "  if(row.word!=''):\n",
        "    X_sentence.append(row.word)\n",
        "    Y_sentence.append(row.tag+'_'+row.POS[0][1])\n",
        "  else:\n",
        "    X.append(X_sentence)\n",
        "    Y.append(Y_sentence)\n",
        "    X_sentence = []\n",
        "    Y_sentence = []"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6xbm9IQKstQ"
      },
      "source": [
        "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
        "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rfcl98SKtzg",
        "outputId": "373db88b-e960-42e3-9abf-7f8f7eb4f22e"
      },
      "source": [
        "print(\"Total number of tagged sentences: {}\".format(len(X)))\n",
        "print(\"Vocabulary size: {}\".format(num_words))\n",
        "print(\"Total number of tags: {}\".format(num_tags))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of tagged sentences: 70103\n",
            "Vocabulary size: 58315\n",
            "Total number of tags: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbDjKmnZKxB7",
        "outputId": "db8522a5-74af-466f-cdaa-b379818bb378"
      },
      "source": [
        "# let's look at first data point\n",
        "# this is one data point that will be fed to the RNN\n",
        "print('sample X: ', X[0], '\\n')\n",
        "print('sample Y: ', Y[0], '\\n')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample X:  ['#', 'منبع', ':', ')', 'مجلة', 'سروش', 'هفتگی', '،', 'مصاحبه', 'با', 'رئیس', 'دفتر', 'الجزیره', 'در', 'تهران', '،', 'یک', 'هزار', 'و', 'سیصد', 'و', 'هشتاد', '(', '#', '#', 'الجزیره', 'هیچ', 'ارتباط', 'خاصی', 'با', 'طالبان', 'ندارد', '.'] \n",
            "\n",
            "sample Y:  ['N_PUNC', 'N_N', 'N_PUNC', 'N_PUNC', 'P_Ne', 'P_N', 'N_AJ', 'N_PUNC', 'N_N', 'N_P', 'P_N', 'P_N', 'N_N', 'N_P', 'N_N', 'N_PUNC', 'N_NUM', 'N_NUM', 'N_CONJ', 'N_NUM', 'N_CONJ', 'N_NUM', 'N_PUNC', 'N_PUNC', 'N_PUNC', 'N_N', 'N_DET', 'P_N', 'N_AJ', 'N_P', 'N_N', 'N_V', 'N_PUNC'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1mxNBZAMVM6",
        "outputId": "e0c0ea82-816f-4939-8885-223e3ad98fc2"
      },
      "source": [
        "# In this many-to-many problem, the length of each input and output sequence must be the same.\n",
        "# Since each word is tagged, it's important to make sure that the length of input sequence equals the output sequence\n",
        "print(\"Length of first input sequence  : {}\".format(len(X[0])))\n",
        "print(\"Length of first output sequence : {}\".format(len(Y[0])))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of first input sequence  : 33\n",
            "Length of first output sequence : 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJQUjpUxManC"
      },
      "source": [
        "# Vectorise X and Y\n",
        "Encode X and Y to integer values\n",
        "\n",
        "We'll use the Tokenizer() function from Keras library to encode text sequence to integer sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXyfPsS0MehY"
      },
      "source": [
        "# encode X\n",
        "\n",
        "word_tokenizer = Tokenizer()                      # instantiate tokeniser\n",
        "word_tokenizer.fit_on_texts(X)                    # fit tokeniser on data\n",
        "X_encoded = word_tokenizer.texts_to_sequences(X)  # use the tokeniser to encode input sequence"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4hmGFQANaFa"
      },
      "source": [
        "# encode Y\n",
        "\n",
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(Y)\n",
        "Y_encoded = tag_tokenizer.texts_to_sequences(Y)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-k8rgpfNdXg",
        "outputId": "6fa80ca3-1b90-4306-9013-7049d37193e4"
      },
      "source": [
        "# look at first encoded data point\n",
        "\n",
        "print(\"** Raw data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
        "print('X: ', X[0], '\\n')\n",
        "print('Y: ', Y[0], '\\n')\n",
        "print()\n",
        "print(\"** Encoded data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
        "print('X: ', X_encoded[0], '\\n')\n",
        "print('Y: ', Y_encoded[0], '\\n')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** Raw data point ** \n",
            " ---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "X:  ['#', 'منبع', ':', ')', 'مجلة', 'سروش', 'هفتگی', '،', 'مصاحبه', 'با', 'رئیس', 'دفتر', 'الجزیره', 'در', 'تهران', '،', 'یک', 'هزار', 'و', 'سیصد', 'و', 'هشتاد', '(', '#', '#', 'الجزیره', 'هیچ', 'ارتباط', 'خاصی', 'با', 'طالبان', 'ندارد', '.'] \n",
            "\n",
            "Y:  ['N_PUNC', 'N_N', 'N_PUNC', 'N_PUNC', 'P_Ne', 'P_N', 'N_AJ', 'N_PUNC', 'N_N', 'N_P', 'P_N', 'P_N', 'N_N', 'N_P', 'N_N', 'N_PUNC', 'N_NUM', 'N_NUM', 'N_CONJ', 'N_NUM', 'N_CONJ', 'N_NUM', 'N_PUNC', 'N_PUNC', 'N_PUNC', 'N_N', 'N_DET', 'P_N', 'N_AJ', 'N_P', 'N_N', 'N_V', 'N_PUNC'] \n",
            "\n",
            "\n",
            "** Encoded data point ** \n",
            " ---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "X:  [30, 668, 12, 43, 1333, 1543, 1585, 3, 1607, 11, 171, 526, 1740, 4, 88, 3, 13, 33, 1, 140, 1, 179, 39, 30, 30, 1740, 92, 413, 640, 11, 4360, 243, 2] \n",
            "\n",
            "Y:  [3, 1, 3, 3, 10, 2, 8, 3, 1, 5, 2, 2, 1, 5, 1, 3, 9, 9, 6, 9, 6, 9, 3, 3, 3, 1, 17, 2, 8, 5, 1, 4, 3] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30AIUVRZNocx",
        "outputId": "a73cd1bc-9560-4671-e92c-8630d9da3ab2"
      },
      "source": [
        "# make sure that each sequence of input and output is same length\n",
        "\n",
        "different_length = [1 if len(input) != len(output) else 0 for input, output in zip(X_encoded, Y_encoded)]\n",
        "print(\"{} sentences have disparate input-output lengths.\".format(sum(different_length)))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 sentences have disparate input-output lengths.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOeKRtn4Nur6"
      },
      "source": [
        "## Pad sequences\n",
        "The next step after encoding the data is to define the sequence lengths. As of now, the sentences present in the data are of various lengths. We need to either pad short sentences or truncate long sentences to a fixed length. This fixed length, however, is a hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVuc9nSbN04B",
        "outputId": "30454b67-9e97-4546-f4c3-daaeed3408c4"
      },
      "source": [
        "# check length of longest sentence\n",
        "lengths = [len(seq) for seq in X_encoded]\n",
        "print(\"Length of longest sentence: {}\".format(max(lengths)))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of longest sentence: 1146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "H8FperICN7sJ",
        "outputId": "4f5d4d01-8797-47e6-9675-796d7c59b2a0"
      },
      "source": [
        "sns.boxplot(lengths)\n",
        "plt.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD4CAYAAADIH9xYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN9klEQVR4nO3da4xc5X2A8efPDl4bKMYs+NIl0mJt3NiSpYb6A6hVhVpDbBSlqmQkoiBvelGlVrLcYqkCYYQt/CVthWpMVRL1onVNQ1qTtgjFRjgNX2ltNWCKwWyIk7DEsdmoIAr4+vbDnN0dG9jx2DuXv/v8pJVnzjk75315D49nzi4iSilIknrfFd0egCTpwhhsSUrCYEtSEgZbkpIw2JKURK2Vg2+44YYyNDTUpqFI0uXpwIED75RSbrzU12kp2ENDQ+zfv/9SzylJ/69ExI9m43W8JSJJSRhsSUrCYEtSEgZbkpIw2JKUhMGWpCQMtiQlYbAlKQmDLUlJGGxJSsJgS1ISBluSkjDYkpSEwZakJAy2JCVhsCUpCYMtSUkYbElKwmBLUhIt/T8dZ8OOHTsYGxtjfHwcgMHBQQCGh4fZsGFDp4cjSWl0PNhjY2N8/5VDQAHg6IkafR/8vNPDkKR0unJL5MxV13PmqgHOXDXAh5+7izNXXd+NYUhSKt7DlqQkDLYkJWGwJSkJgy1JSRhsSUrCYEtSEgZbkpIw2JKUhMGWpCQMtiQlYbAlKQmDLUlJGGxJSsJgS1ISBluSkjDYkpSEwZakJAy2JCVhsCUpCYMtSUkYbElKwmBLUhIGW5KSMNiSlITBlqQkDLYkJWGwJSkJgy1JSRhsSUrCYEtSEgZbkpIw2JKUhMGWpCQMtiQlYbAlKQmDLUlJGGxJSsJgS1ISBluSkjDYkpSEwZakJDoS7B07drBjx46ufb8kXQ5qnTjJ2NhYV79fki4H3hKRpCQMtiQlYbAlKQmDLUlJGGxJSsJgS1ISBluSkjDYkpSEwZakJAy2JCVhsCUpCYMtSUkYbElKwmBLUhIGW5KSMNiSlITBlqQkDLYkJWGwJSkJgy1JSRhsSUrCYEtSEgZbkpIw2JKUhMGWpCQMtiQlYbAlKQmDLUlJGGxJSsJgS1ISBluSkjDYkpSEwZakJAy2JCVhsCUpCYMtSUkYbElKwmBLUhK1bg/gQrz00ksA3H777R097/z583n33XcBiAhKKVP71q1bx9NPP02tVuPUqVMMDg5y7NgxTp06NXXMpk2beOqppxgfH2fx4sVce+21AJw8eZK3336bkydPnnO+/v5+7r77bnbt2sXChQvPOffjjz/OggUL2Lp1K+vXr+ehhx5iyZIl1Go1+vr62LRpE4899hgPP/wwAwMDAExMTLB169am22bDxMQEmzdvJiJ45JFHmr52s3HMtL+VObTr2HbphTFoWqvXdbv5DnsGk8EEzok1wO7duymlTAV6fHz8nFgDPProo4yPjwNw9OhRDh8+zOHDhzly5MjHYg1w4sQJdu3aBcCxY8c4ceIEJ06c4KOPPmLbtm2Mjo5y8OBBtmzZwocffsibb77J4cOHOXToENu2bePgwYPs3Llz6vUmj2+2bTaMjo5y6NAhXn311Qt67WbjmGl/K3No17Ht0gtj0LRWr+t26/lgd/pd9Ww6P/KX4siRI+zZs4dSCu+///4n7i+lsHfvXiYmJpiYmGDv3r1Nt82GiYkJ9uzZM/V8z549M752s3HMtL+VObTr2HbphTFoWqvXdSd0JNjj4+OMjY2xceNGxsbGuOKj984dxEfvTe0//0vTzn8H/0nOnDnDzp07GR0d5ezZs023zYbR0VFOnz59zjhneu1m45hpfytzaNex7dILY9C0Vq/rTmga7Ij4g4jYHxH7jx8/3okx6RKcPn2a559/nn379k1dbDNtmw379u0759NEKWXG1242jpn2tzKHdh3bLr0wBk1r9bruhKbBLqV8o5SyqpSy6sYbb7yokwwODjI8PMz27dsZHh7m7Nxrz9l/du61U/vP/1JrarUad9xxB6tXr6ZWqzXdNhtWr15NREw9j4gZX7vZOGba38oc2nVsu/TCGDSt1eu6E3r+HramXXnllU2P6evrY/369YyMjHDFFVc03TYbRkZGpkIzOc6ZXrvZOGba38oc2nVsu/TCGDSt1eu6E3o+2C+88EK3h3DRGv92vlRDQ0OsXbuWiOCaa675xP0RwZo1axgYGGBgYIA1a9Y03TYbBgYGWLt27dTztWvXzvjazcYx0/5W5tCuY9ulF8agaa1e153Q88Hupvnz5089Pj++69atIyKm3vUODg5+7B3wfffdx+DgIACLFy9m2bJlLFu2jKGhIebMmfOx8/X393PvvfcCsHDhQvr7++nv72fu3Lls3ryZkZERVq5cyZYtW5g3bx5Lly5l2bJlLF++nM2bN7Ny5cqPvRu9kG2zYWRkhOXLl7NixYoLeu1m45hpfytzaNex7dILY9C0Vq/rdotWfvVs1apVZf/+/S2fZPK3PbZv387GjRs58ObPpvZ9+Lm7mPfad/iVpYs+9Z514/dLUjYRcaCUsupSX8d32JKUhMGWpCQMtiQlYbAlKQmDLUlJGGxJSsJgS1ISBluSkjDYkpSEwZakJAy2JCVhsCUpCYMtSUkYbElKwmBLUhIGW5KSMNiSlITBlqQkDLYkJWGwJSkJgy1JSRhsSUrCYEtSEgZbkpIw2JKUhMGWpCQMtiQlYbAlKQmDLUlJGGxJSsJgS1ISBluSkjDYkpSEwZakJAy2JCVhsCUpCYMtSUkYbElKotaJkwwPD3f1+yXpctCRYG/YsKGr3y9JlwNviUhSEgZbkpIw2JKUhMGWpCQMtiQlYbAlKQmDLUlJGGxJSsJgS1ISBluSkjDYkpSEwZakJAy2JCVhsCUpCYMtSUkYbElKwmBLUhIGW5KSMNiSlITBlqQkDLYkJWGwJSkJgy1JSRhsSUrCYEtSEgZbkpIw2JKUhMGWpCQMtiQlYbAlKQmDLUlJGGxJSsJgS1ISBluSkjDYkpSEwZakJAy2JCVhsCUpCYMtSUkYbElKotaNk/Z98HOgADDvte9Uzxd1YyiSlEbHgz08PAzA+Pg4AIODi4BFU9slSZ+s48HesGFDp08pSZcF72FLUhIGW5KSMNiSlITBlqQkDLYkJWGwJSkJgy1JSRhsSUrCYEtSEgZbkpIw2JKUhMGWpCQMtiQlYbAlKQmDLUlJGGxJSsJgS1ISBluSkjDYkpSEwZakJKKUcuEHRxwHfnSR57oBeOciv7dXOaccnFMOl+OcoD6vq0spN17qC7UU7Es6UcT+UsqqjpysQ5xTDs4ph8txTjC78/KWiCQlYbAlKYlOBvsbHTxXpzinHJxTDpfjnGAW59Wxe9iSpEvjLRFJSsJgS1ISbQ92RKyJiNcjYiwi7m/3+WZLRHwmIr4XEa9GxH9HxMZq+/UR8XxEvFH9uaDaHhHxWDXPlyPilu7O4NNFRF9E/FdEPFs9vzkiXqzG/q2ImFNt76+ej1X7h7o57k8TEddFxO6IeC0iDkXEbZfJOv1Jde29EhHfjIi52dYqIv4uIo5FxCsN21pem4gYqY5/IyJGujGXhrF80pz+vLr+Xo6If4mI6xr2PVDN6fWI+ELD9tbbWEpp2xfQB/wAWArMAV4CVrTznLM49iXALdXjXwAOAyuAPwPur7bfD3ytenwXsAcI4FbgxW7PYYa53Qf8I/Bs9fyfgHuqx08Af1g9/iPgierxPcC3uj32T5nPKPD71eM5wHXZ1wkYBH4IzGtYo69mWyvg14FbgFcatrW0NsD1wJvVnwuqxwt6bE53ArXq8dca5rSi6l4/cHPVw76LbWO7J3Yb8FzD8weAB7p9EV3kXP4NuAN4HVhSbVsCvF49/jrw5Ybjp47rpS/gJuC7wG8Az1b/crzTcLFNrRnwHHBb9bhWHRfdnsN585lfhS3O2559nQaBn1SRqlVr9YWMawUMnRe3ltYG+DLw9Ybt5xzXC3M6b99vA09Wj89p3uQ6XWwb231LZPKim/RWtS2V6uPl54EXgUWllJ9Wu44Ci6rHWeb6l8CfAmer5wPA/5RSTlfPG8c9Nadq/7vV8b3kZuA48PfVbZ6/iYirSb5OpZRx4C+AHwM/pf7P/gC512pSq2uTYs0a/C71Twowy3Pyh45NRMQ1wNPAH5dS3mvcV+p/Nab5vciI+CJwrJRyoNtjmUU16h9P/7qU8nngf6l/zJ6SbZ0Aqvu6v0X9L6RfBK4G1nR1UG2QcW1mEhEPAqeBJ9vx+u0O9jjwmYbnN1XbUoiIK6nH+slSyrerzT+LiCXV/iXAsWp7hrn+KvCliDgCPEX9tsh24LqIqFXHNI57ak7V/vnARCcHfAHeAt4qpbxYPd9NPeCZ1wlgNfDDUsrxUsop4NvU1y/zWk1qdW1SrFlEfBX4IvCV6i8imOU5tTvY/wl8tvrJ9hzqPwx5ps3nnBUREcDfAodKKY827HoGmPwp9Qj1e9uT29dXP+m+FXi34WNfTyilPFBKuamUMkR9Lf69lPIV4HvAuuqw8+c0Odd11fE99W6olHIU+ElE/FK16TeBV0m8TpUfA7dGxFXVtTg5r7Rr1aDVtXkOuDMiFlSfPO6stvWMiFhD/Vbjl0opHzTsega4p/otnpuBzwL/wcW2sQM35++i/hsWPwAe7OYPCloc969R/6j2MvD96usu6vcFvwu8AewDrq+OD+CvqnkeBFZ1ew5N5nc7078lsrS6iMaAfwb6q+1zq+dj1f6l3R73p8zll4H91Vr9K/XfJEi/TsBW4DXgFeAfqP+mQaq1Ar5J/R78Keqfhn7vYtaG+n3hserrd3pwTmPU70lPtuKJhuMfrOb0OrC2YXvLbfQ/TZekJPyhoyQlYbAlKQmDLUlJGGxJSsJgS1ISBluSkjDYkpTE/wGkV9r6uClZJQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYlnqGXMOEu5"
      },
      "source": [
        "# Pad each sequence to MAX_SEQ_LENGTH using KERAS' pad_sequences() function. \n",
        "# Sentences longer than MAX_SEQ_LENGTH are truncated.\n",
        "# Sentences shorter than MAX_SEQ_LENGTH are padded with zeroes.\n",
        "\n",
        "# Truncation and padding can either be 'pre' or 'post'. \n",
        "# For padding we are using 'pre' padding type, that is, add zeroes on the left side.\n",
        "# For truncation, we are using 'post', that is, truncate a sentence from right side.\n",
        "\n",
        "MAX_SEQ_LENGTH = 100  # sequences greater than 100 in length will be truncated\n",
        "\n",
        "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
        "Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTTIopW-OH86",
        "outputId": "e20008a1-02f3-4974-8244-16b707a52d5b"
      },
      "source": [
        "# print the first sequence\n",
        "print(X_padded[0], \"\\n\"*3)\n",
        "print(Y_padded[0])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0   30  668   12\n",
            "   43 1333 1543 1585    3 1607   11  171  526 1740    4   88    3   13\n",
            "   33    1  140    1  179   39   30   30 1740   92  413  640   11 4360\n",
            "  243    2] \n",
            "\n",
            "\n",
            "\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  1  3  3 10\n",
            "  2  8  3  1  5  2  2  1  5  1  3  9  9  6  9  6  9  3  3  3  1 17  2  8\n",
            "  5  1  4  3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iJUxDSFOOXS"
      },
      "source": [
        "RNN will learn the zero to zero mapping while training. So we don't need to worry about the padded zeroes. Please note that zero is not reserved for any word or tag, it's only reserved for padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2aYIfyrOQER"
      },
      "source": [
        "# assign padded sequences to X and Y\n",
        "X, Y = X_padded, Y_padded"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ciaWlpHOd2h"
      },
      "source": [
        "## Word embeddings\n",
        "Currently, each word and each tag is encoded as an integer.\n",
        "\n",
        "We'll use a more sophisticated technique to represent the input words (X) using what's known as word embeddings.\n",
        "\n",
        "However, to represent each tag in Y, we'll simply use one-hot encoding scheme since there are only 13 tags in the dataset and the LSTM will have no problems in learning its own representation of these tags.\n",
        "\n",
        "To use word embeddings, you can go for either of the following models:\n",
        "\n",
        "1. word2vec model: https://code.google.com/archive/p/word2vec/\n",
        "2. GloVe model : https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "We're using the word2vec model for no particular reason. Both of these are very efficient in representing words. You can try both and see which one works better.\n",
        "\n",
        "Dimensions of a word embedding is: (VOCABULARY_SIZE, EMBEDDING_DIMENSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT1N0ryAO_f6"
      },
      "source": [
        "## Use word embeddings for input sequences (X)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y9WZGiaPCD5"
      },
      "source": [
        "# word2vec\n",
        "\n",
        "path = '/content/word2vec.model-cbow-size=200-window=5.bin'\n",
        "\n",
        "\n",
        "# load word2vec using the following function present in the gensim library\n",
        "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8WmHF0PPTWp",
        "outputId": "94ebc59f-ad26-4ad6-ee17-9a7173c0f475"
      },
      "source": [
        "# word2vec effectiveness\n",
        "word2vec.most_similar(positive = [\"شاه\", \"زن\"], negative = [\"مرد\"])"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('احمدشاه', 0.5828825235366821),\n",
              " ('پادشاه', 0.5697827339172363),\n",
              " ('محمدشاه', 0.5616036653518677),\n",
              " ('ناصرالدین\\u200cشاه', 0.5586493015289307),\n",
              " ('ولیعهد', 0.5397764444351196),\n",
              " ('فتحعلی\\u200cشاه', 0.5261922478675842),\n",
              " ('شاه\\u200cعباس', 0.5197480320930481),\n",
              " ('گوکبوری', 0.5167436003684998),\n",
              " ('دربار', 0.5143003463745117),\n",
              " ('رضاشاه', 0.5133230686187744)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdd6rAVwPVV1"
      },
      "source": [
        "# assign word vectors from word2vec model\n",
        "\n",
        "EMBEDDING_SIZE  = 200  # each word in word2vec model is represented using a 200 dimensional vector\n",
        "VOCABULARY_SIZE = len(word_tokenizer.word_index) + 1\n",
        "\n",
        "# create an empty embedding matix\n",
        "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
        "\n",
        "# create a word to index dictionary mapping\n",
        "word2id = word_tokenizer.word_index\n",
        "\n",
        "# copy vectors from word2vec model to the words present in corpus\n",
        "for word, index in word2id.items():\n",
        "    try:\n",
        "        embedding_weights[index, :] = word2vec[word]\n",
        "    except KeyError:\n",
        "        pass"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydNPV52WVTHM",
        "outputId": "65023cc3-4ca1-4bc6-9f47-8249abae8dc8"
      },
      "source": [
        "# check embedding dimension\n",
        "print(\"Embeddings shape: {}\".format(embedding_weights.shape))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embeddings shape: (58316, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGXwL5zYVbbk",
        "outputId": "2c00e54c-6140-436c-d63e-12f0c02a2048"
      },
      "source": [
        "# let's look at an embedding of a word\n",
        "embedding_weights[word_tokenizer.word_index['کتاب']]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.27324462e+00,  6.19029880e-01, -1.89760199e-03, -1.62530839e-01,\n",
              "       -3.40133309e-01,  7.15490803e-03,  4.38465714e-01, -5.92125878e-02,\n",
              "       -9.68807340e-01, -1.38433576e+00, -3.04762578e+00,  2.95990467e-01,\n",
              "        1.07385790e+00, -1.15435159e+00, -5.31624436e-01, -2.56763846e-01,\n",
              "        1.29102254e+00, -2.81041771e-01,  1.29225636e+00, -8.90765548e-01,\n",
              "       -1.99803841e+00,  1.14900935e+00, -2.53689557e-01,  3.02091409e-02,\n",
              "       -2.15287280e+00, -1.12354589e+00,  3.31524515e+00, -1.05865049e+00,\n",
              "        2.65959835e+00,  3.50926667e-01, -1.17137015e+00, -9.45076823e-01,\n",
              "        5.91018684e-02,  3.41299987e+00, -1.32753825e+00, -1.15575337e+00,\n",
              "       -1.99846387e+00, -2.20947957e+00,  9.55326781e-02, -1.91129196e+00,\n",
              "       -6.60164595e-01,  1.47260499e+00, -3.27217221e+00, -3.76167226e+00,\n",
              "        8.92697334e-01, -9.27080929e-01,  1.62479258e+00, -6.52753651e-01,\n",
              "        1.76814890e+00, -2.29427290e+00, -1.17490709e+00, -4.73752052e-01,\n",
              "       -2.28890419e-01,  2.33515769e-01,  2.21023607e+00, -1.21030498e+00,\n",
              "       -3.53014439e-01, -5.42405486e-01,  1.23781776e+00, -2.80707192e+00,\n",
              "        4.07723308e-01,  5.76767266e-01, -3.75963189e-02,  6.49117291e-01,\n",
              "        1.10293996e+00,  1.25480819e+00, -1.30161121e-01,  2.67351532e+00,\n",
              "        2.39370012e+00, -1.50226021e+00,  1.36713952e-01,  4.69496667e-01,\n",
              "       -2.54423022e-01, -2.09923553e+00,  6.37481153e-01,  6.78151429e-01,\n",
              "        1.85932434e+00,  1.72726119e+00, -1.80076420e+00,  3.07502627e-01,\n",
              "        4.82240081e-01, -1.82519853e+00, -4.15479779e-01, -1.15819812e+00,\n",
              "       -3.39726925e+00,  1.18938267e+00,  6.77953959e-01,  1.80151200e+00,\n",
              "       -2.01316023e+00,  1.59018946e+00, -6.77523077e-01, -5.65155685e-01,\n",
              "       -1.10347414e+00, -1.81443632e+00, -1.33020449e+00, -1.01113427e+00,\n",
              "        6.71847880e-01,  2.47895017e-01, -1.22860432e+00, -1.44988143e+00,\n",
              "       -2.03730440e+00,  2.30442190e+00, -1.69966400e+00,  8.91804874e-01,\n",
              "       -1.66169465e+00, -2.56529927e+00,  5.42982042e-01,  7.12270737e-01,\n",
              "        3.47609019e+00,  3.52390170e+00,  2.61290967e-01, -1.60075784e+00,\n",
              "        6.36892736e-01,  6.94596469e-01, -1.20669758e+00, -3.57153177e-01,\n",
              "        1.00866802e-01, -5.36761880e-01, -1.87106299e+00, -2.44349703e-01,\n",
              "       -2.97795796e+00, -1.61531639e+00,  9.48838711e-01, -1.23766577e+00,\n",
              "        5.27903318e-01, -9.25092995e-02,  3.05890965e+00,  3.73449653e-01,\n",
              "        1.03258908e+00,  3.04773307e+00, -3.16021228e+00, -5.25500715e-01,\n",
              "       -8.74297798e-01, -1.56509876e+00,  2.32160282e+00,  2.49220824e+00,\n",
              "        1.84219372e+00,  3.12515467e-01, -7.22577155e-01, -1.09589851e+00,\n",
              "       -2.02415466e-01, -7.39798009e-01,  7.53901064e-01,  9.97188449e-01,\n",
              "       -1.83116746e+00, -1.01811361e+00,  2.08861724e-01,  1.62463093e+00,\n",
              "        3.05226771e-03, -7.15499997e-01, -3.80607069e-01, -1.07576990e+00,\n",
              "       -4.70391124e-01, -1.60197079e+00,  1.50102243e-01, -1.44984865e+00,\n",
              "        3.99166578e-03, -7.31847584e-01,  2.00313401e+00, -1.91264641e+00,\n",
              "        5.44874012e-01,  1.18001497e+00, -1.97201216e+00,  2.93949747e+00,\n",
              "       -8.74565721e-01, -7.88469851e-01,  4.50353801e-01,  6.95267975e-01,\n",
              "        2.61395305e-01, -3.57018042e+00, -2.72543716e+00, -3.22682485e-02,\n",
              "       -1.18047714e+00,  1.32266149e-01, -1.01196480e+00,  7.97137856e-01,\n",
              "        1.21913992e-01, -3.23858118e+00,  2.26848817e+00,  7.98443258e-02,\n",
              "       -1.32389128e+00,  1.82927704e+00,  1.14047027e+00,  9.77619231e-01,\n",
              "        1.22888923e+00,  2.87990618e+00, -1.21843314e+00,  1.52390301e+00,\n",
              "       -1.65527368e+00,  5.93187928e-01, -2.33278084e+00,  2.64765668e+00,\n",
              "       -8.36802542e-01, -4.76327948e-02,  7.68921852e-01,  5.80105603e-01,\n",
              "        9.66356933e-01,  7.25558341e-01,  1.93639266e+00, -8.37439775e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89jSwF2dVkBl"
      },
      "source": [
        "## Use one-hot encoding for output sequences (Y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g711V3KVmRr"
      },
      "source": [
        "# use Keras' to_categorical function to one-hot encode Y\n",
        "Y = to_categorical(Y)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZO60CT0Vrvc",
        "outputId": "5478bf3f-adcf-417f-c520-44161e13c662"
      },
      "source": [
        "# print Y of the first output sequqnce\n",
        "print(Y.shape)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70103, 100, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXMZt-hRVyQE"
      },
      "source": [
        "# Split data in training, validation and tesing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hz_5S-IV1cr"
      },
      "source": [
        "# split entire data into training and testing sets\n",
        "TEST_SIZE = 0.15\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=4)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1zTc_cjV7rO"
      },
      "source": [
        "# split training data into training and validation sets\n",
        "VALID_SIZE = 0.15\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-pnUPniV-gT",
        "outputId": "c13e2fce-c98d-4398-efa2-dda6f12518bc"
      },
      "source": [
        "# print number of samples in each set\n",
        "print(\"TRAINING DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_train.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_train.shape))\n",
        "print(\"-\"*50)\n",
        "print(\"VALIDATION DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_validation.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_validation.shape))\n",
        "print(\"-\"*50)\n",
        "print(\"TESTING DATA\")\n",
        "print('Shape of input sequences: {}'.format(X_test.shape))\n",
        "print('Shape of output sequences: {}'.format(Y_test.shape))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING DATA\n",
            "Shape of input sequences: (50648, 100)\n",
            "Shape of output sequences: (50648, 100, 40)\n",
            "--------------------------------------------------\n",
            "VALIDATION DATA\n",
            "Shape of input sequences: (8939, 100)\n",
            "Shape of output sequences: (8939, 100, 40)\n",
            "--------------------------------------------------\n",
            "TESTING DATA\n",
            "Shape of input sequences: (10516, 100)\n",
            "Shape of output sequences: (10516, 100, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8nknKNYWHg_"
      },
      "source": [
        "# Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB7dDGZnWN5D"
      },
      "source": [
        "## Create model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLy5mjfaWZUr"
      },
      "source": [
        "# total number of tags\n",
        "NUM_CLASSES = Y.shape[2]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cBWKFWoWJs9"
      },
      "source": [
        "# create architecture\n",
        "\n",
        "bidirect_model = Sequential()\n",
        "bidirect_model.add(Embedding(input_dim     = VOCABULARY_SIZE,\n",
        "                             output_dim    = EMBEDDING_SIZE,\n",
        "                             input_length  = MAX_SEQ_LENGTH,\n",
        "                             weights       = [embedding_weights],\n",
        "                             trainable     = True\n",
        "))\n",
        "bidirect_model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "bidirect_model.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awI97SNCWiLN"
      },
      "source": [
        "## Compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EVoYrxQWj8M"
      },
      "source": [
        "bidirect_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfcHUOxIWpPk",
        "outputId": "5c7b39ad-17fe-46aa-ea99-f7130ec6b9a0"
      },
      "source": [
        "# check summary of model\n",
        "bidirect_model.summary()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 200)          11663200  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 100, 128)          135680    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 100, 40)           5160      \n",
            "=================================================================\n",
            "Total params: 11,804,040\n",
            "Trainable params: 11,804,040\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IpziCtaWtfE"
      },
      "source": [
        "## Fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpEIou9yWvAD",
        "outputId": "2573f4bf-5441-448e-d0a5-baab9c498a59"
      },
      "source": [
        "bidirect_training = bidirect_model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_validation, Y_validation))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "396/396 [==============================] - 278s 645ms/step - loss: 0.7337 - acc: 0.8968 - val_loss: 0.0607 - val_acc: 0.9825\n",
            "Epoch 2/10\n",
            "396/396 [==============================] - 255s 644ms/step - loss: 0.0487 - acc: 0.9859 - val_loss: 0.0327 - val_acc: 0.9900\n",
            "Epoch 3/10\n",
            "396/396 [==============================] - 255s 645ms/step - loss: 0.0264 - acc: 0.9918 - val_loss: 0.0257 - val_acc: 0.9918\n",
            "Epoch 4/10\n",
            "278/396 [====================>.........] - ETA: 1:13 - loss: 0.0188 - acc: 0.9942"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}